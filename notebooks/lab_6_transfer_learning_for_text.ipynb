{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning for Text Data\n",
    "\n",
    "In this lab, we begin an exploration of transfer learning models designed to facilitate multilingual modeling. \n",
    "\n",
    "First, make sure you have the following dependencies:\n",
    "\n",
    "```bash\n",
    "    $ pip install torch tensorflow transformers cld2-cffi\n",
    "```\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cld2\n",
    "import gzip\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import simplejson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and load the datasets\n",
    "\n",
    "This project requires two datasets, both containing reviews of books. The first dataset contains Hindi-language book reviews, and was originally gathered from Raghvendra Pratap Singh\n",
    "([MrRaghav](https://github.com/MrRaghav)) via [his GitHub repository](https://github.com/MrRaghav/Complaints-mining-from-Hindi-product-reviews) concerning complaint-mining in product reviews.\n",
    "\n",
    "\n",
    "The second dataset contains English-language book reviews, and is a subset of the [Amazon product review corpus](https://registry.opendata.aws/amazon-reviews-ml/), a (unfortunately English-only, to my knowledge) portion of which is available from [Julian McAuley at UCSD](https://cseweb.ucsd.edu/~jmcauley/) [here](http://jmcauley.ucsd.edu/data/amazon/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(url, fname):\n",
    "    \"\"\"\n",
    "    Helper method to retrieve data via Python's requests module\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    outpath  = os.path.abspath(fname)\n",
    "    with open(outpath, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    \n",
    "    return outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the Hindi review data\n",
    "\n",
    "FIXTURES = os.path.join(\"..\", \"data\")\n",
    "\n",
    "if not os.path.exists(FIXTURES):\n",
    "    os.makedirs(FIXTURES)\n",
    "\n",
    "HINDI_FILE = os.path.join(FIXTURES, \"amazon-youtube-hindi-complaints-data.xlsx\")\n",
    "HINDI_URL = \"https://tinyurl.com/y5h2dkn8\"\n",
    "HINDI_REVIEWS = fetch_data(HINDI_URL, HINDI_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Label</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>वीवो वी 19 अच्छा है इनका गैलरी मजा नहीं आता  स...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phone</td>\n",
       "      <td>0</td>\n",
       "      <td>बहोत सस्ता है</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Book</td>\n",
       "      <td>0</td>\n",
       "      <td>किंडल आपके साथ इस किताब को पढ़ने में मुझे कंटि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Book</td>\n",
       "      <td>0</td>\n",
       "      <td>मुस्लिम शासकों उनके अत्याचारों से हिन्दू जनता ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Book</td>\n",
       "      <td>0</td>\n",
       "      <td>पर नशा है आईएएस की तैयारी</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category  Label                                            Reviews\n",
       "0    Phone      1  वीवो वी 19 अच्छा है इनका गैलरी मजा नहीं आता  स...\n",
       "1    Phone      0                                     बहोत सस्ता है \n",
       "2     Book      0  किंडल आपके साथ इस किताब को पढ़ने में मुझे कंटि...\n",
       "3     Book      0  मुस्लिम शासकों उनके अत्याचारों से हिन्दू जनता ...\n",
       "4     Book      0                          पर नशा है आईएएस की तैयारी"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_reviews = pd.read_excel(\n",
    "    HINDI_REVIEWS, \n",
    "    sheet_name=\"Sheet1\"\n",
    ")\n",
    "hindi_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2839\n"
     ]
    }
   ],
   "source": [
    "# This dataset includes both book and phone reviews.\n",
    "# Let's keep only the book reviews.\n",
    "\n",
    "hindi_reviews = hindi_reviews[hindi_reviews.Category == \"Book\"]\n",
    "hindi_reviews = hindi_reviews.drop(columns=[\"Category\"])\n",
    "print(len(hindi_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>किंडल आपके साथ इस किताब को पढ़ने में मुझे कंटि...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>मुस्लिम शासकों उनके अत्याचारों से हिन्दू जनता ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>पर नशा है आईएएस की तैयारी</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>एकदम जबरदस्त किताब है</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>एक जबरदस्त कहानी</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                            Reviews\n",
       "2      0  किंडल आपके साथ इस किताब को पढ़ने में मुझे कंटि...\n",
       "3      0  मुस्लिम शासकों उनके अत्याचारों से हिन्दू जनता ...\n",
       "4      0                          पर नशा है आईएएस की तैयारी\n",
       "5      0                              एकदम जबरदस्त किताब है\n",
       "6      0                                   एक जबरदस्त कहानी"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hindi_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll load the English language reviews\n",
    "# Note that we've previously downloaded them from the link below\n",
    "# http://jmcauley.ucsd.edu/data/amazon/\n",
    "# It's a 3 gig file, compressed\n",
    "\n",
    "def parse(path, n_rows=10000):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    idx = 0\n",
    "    for line in g:\n",
    "        if idx > n_rows:\n",
    "            break\n",
    "        else:\n",
    "            idx += 1\n",
    "            yield eval(line)\n",
    "\n",
    "def make_dataframe(path):\n",
    "    idx = 0\n",
    "    df = {}\n",
    "    for dictionary in parse(path):\n",
    "        df[idx] = dictionary\n",
    "        idx += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A10000012B7CGYKOMPQ4L</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>Adam</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>Spiritually and mentally inspiring! A book tha...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Wonderful!</td>\n",
       "      <td>1355616000</td>\n",
       "      <td>12 16, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A2S166WSCFIFP5</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>adead_poet@hotmail.com \"adead_poet@hotmail.com\"</td>\n",
       "      <td>[0, 2]</td>\n",
       "      <td>This is one my must have books. It is a master...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>close to god</td>\n",
       "      <td>1071100800</td>\n",
       "      <td>12 11, 2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1BM81XB4QHOA3</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>Ahoro Blethends \"Seriously\"</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>This book provides a reflection that you can a...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Must Read for Life Afficianados</td>\n",
       "      <td>1390003200</td>\n",
       "      <td>01 18, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1MOSTXNIO5MPJ</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>Alan Krug</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>I first read THE PROPHET in college back in th...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Timeless for every good and bad time in your l...</td>\n",
       "      <td>1317081600</td>\n",
       "      <td>09 27, 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2XQ5LZHTD4AFT</td>\n",
       "      <td>000100039X</td>\n",
       "      <td>Alaturka</td>\n",
       "      <td>[7, 9]</td>\n",
       "      <td>A timeless classic.  It is a very demanding an...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>A Modern Rumi</td>\n",
       "      <td>1033948800</td>\n",
       "      <td>10 7, 2002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              reviewerID        asin  \\\n",
       "0  A10000012B7CGYKOMPQ4L  000100039X   \n",
       "1         A2S166WSCFIFP5  000100039X   \n",
       "2         A1BM81XB4QHOA3  000100039X   \n",
       "3         A1MOSTXNIO5MPJ  000100039X   \n",
       "4         A2XQ5LZHTD4AFT  000100039X   \n",
       "\n",
       "                                      reviewerName helpful  \\\n",
       "0                                             Adam  [0, 0]   \n",
       "1  adead_poet@hotmail.com \"adead_poet@hotmail.com\"  [0, 2]   \n",
       "2                      Ahoro Blethends \"Seriously\"  [0, 0]   \n",
       "3                                        Alan Krug  [0, 0]   \n",
       "4                                         Alaturka  [7, 9]   \n",
       "\n",
       "                                          reviewText  overall  \\\n",
       "0  Spiritually and mentally inspiring! A book tha...      5.0   \n",
       "1  This is one my must have books. It is a master...      5.0   \n",
       "2  This book provides a reflection that you can a...      5.0   \n",
       "3  I first read THE PROPHET in college back in th...      5.0   \n",
       "4  A timeless classic.  It is a very demanding an...      5.0   \n",
       "\n",
       "                                             summary  unixReviewTime  \\\n",
       "0                                         Wonderful!      1355616000   \n",
       "1                                       close to god      1071100800   \n",
       "2                    Must Read for Life Afficianados      1390003200   \n",
       "3  Timeless for every good and bad time in your l...      1317081600   \n",
       "4                                      A Modern Rumi      1033948800   \n",
       "\n",
       "    reviewTime  \n",
       "0  12 16, 2012  \n",
       "1  12 11, 2003  \n",
       "2  01 18, 2014  \n",
       "3  09 27, 2011  \n",
       "4   10 7, 2002  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENGL_REVIEWS = os.path.join(FIXTURES, \"reviews_Books_5.json.gz\")\n",
    "english_reviews = make_dataframe(ENGL_REVIEWS)\n",
    "english_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_complaints(rating):\n",
    "    if rating > 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_reviews[\"Score\"] = english_reviews[\"overall\"].apply(get_complaints)\n",
    "\n",
    "english_reviews = english_reviews.drop(\n",
    "    columns=[\n",
    "        \"reviewerID\", \"asin\", \"reviewerName\", \"helpful\", \n",
    "        \"summary\", \"unixReviewTime\", \"reviewTime\", \"overall\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spiritually and mentally inspiring! A book tha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is one my must have books. It is a master...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This book provides a reflection that you can a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I first read THE PROPHET in college back in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A timeless classic.  It is a very demanding an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews  Label\n",
       "0  Spiritually and mentally inspiring! A book tha...      0\n",
       "1  This is one my must have books. It is a master...      0\n",
       "2  This book provides a reflection that you can a...      0\n",
       "3  I first read THE PROPHET in college back in th...      0\n",
       "4  A timeless classic.  It is a very demanding an...      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_reviews.columns = [\"Reviews\", \"Label\"]\n",
    "english_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model Architecture\n",
    "\n",
    "The number of epochs, maximum length of sequences, batch size, and random seed for training are global variables, as is the path to the directory where we will store the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = 38\n",
    "\n",
    "STORE_PATH = os.path.join(\"..\", \"results\")\n",
    "\n",
    "if not os.path.exists(STORE_PATH):\n",
    "    os.makedirs(STORE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(df):\n",
    "    \"\"\"\n",
    "    This prep function will take the feature dataframe as input,\n",
    "    perform tokenization, and return the encoded feature vectors\n",
    "    \"\"\"\n",
    "    sentences = df.values\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-multilingual-cased', do_lower_case=True\n",
    "    )\n",
    "    \n",
    "    encoded_sentences = []\n",
    "    for sent in sentences:\n",
    "        encoded_sent = tokenizer.encode(\n",
    "            sent,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "        \n",
    "        encoded_sentences.append(encoded_sent)\n",
    "\n",
    "    encoded_sentences = pad_sequences(\n",
    "        encoded_sentences, \n",
    "        maxlen=MAX_LEN, \n",
    "        dtype=\"long\", \n",
    "        value=0, \n",
    "        truncating=\"post\", \n",
    "        padding=\"post\"\n",
    "    )\n",
    "\n",
    "    return encoded_sentences\n",
    "\n",
    "\n",
    "def attn_mask(encoded_sentences):\n",
    "    \"\"\"\n",
    "    This function takes the encoded sentences as input and returns \n",
    "    attention masks ahead of BERT training. \n",
    "    \n",
    "    A 0 value corresponds to padding, and a value of 1 is an actual token.\n",
    "    \"\"\"\n",
    "\n",
    "    attention_masks = []\n",
    "    for sent in encoded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        attention_masks.append(att_mask)\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data and preprocess it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = english_reviews[\"Reviews\"]\n",
    "y = english_reviews[\"Label\"]\n",
    "\n",
    "# Create train and test splits\n",
    "X_train, X_test, y_train, y_test = tts(\n",
    "    X, y, test_size=0.20, random_state=38, shuffle=True\n",
    ")\n",
    "\n",
    "X_train_encoded = prep(X_train)\n",
    "X_train_masks = attn_mask(X_train_encoded)\n",
    "\n",
    "X_test_encoded = prep(X_test)\n",
    "X_test_masks = attn_mask(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the input layer to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(X_train_encoded)\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "train_masks = torch.tensor(X_train_masks)\n",
    "\n",
    "validation_inputs = torch.tensor(X_test_encoded)\n",
    "validation_labels = torch.tensor(y_test.values)\n",
    "validation_masks = torch.tensor(X_test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure data loaders for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader for training\n",
    "train_data = TensorDataset(\n",
    "    train_inputs, \n",
    "    train_masks, \n",
    "    train_labels\n",
    ")\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "trainer = DataLoader(\n",
    "    train_data, \n",
    "    sampler=train_sampler, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# data loader for validation\n",
    "validation_data = TensorDataset(\n",
    "    validation_inputs, \n",
    "    validation_masks, \n",
    "    validation_labels\n",
    ")\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validator = DataLoader(\n",
    "    validation_data, \n",
    "    sampler=validation_sampler, \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    num_labels=2,   # we are doing binary classification\n",
    "    output_attentions=False, \n",
    "    output_hidden_states=False, \n",
    ")\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=3e-5, \n",
    "    eps=1e-8,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "total_steps = len(trainer) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See Hugging Face example: https://tinyurl.com/y5629dsp\n",
    "\n",
    "def compute_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Comput the accuracy of the predicted values\n",
    "    \"\"\"\n",
    "    predicted = np.argmax(y_pred, axis=1).flatten()\n",
    "    actual = y_true.flatten()\n",
    "    return np.sum(predicted==actual)/len(actual)\n",
    "\n",
    "\n",
    "def train_model(train_loader, test_loader, epochs):\n",
    "    losses = []\n",
    "    for e in range(epochs):\n",
    "        print('======== Epoch {:} / {:} ========'.format(e + 1, epochs))\n",
    "        start_train_time = time.time()\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "\n",
    "            if step%10 == 0:\n",
    "                elapsed = time.time() - start_train_time\n",
    "                print(\n",
    "                    \"{}/{} --> Time elapsed {}\".format(\n",
    "                        step, len(train_loader), elapsed\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            input_data, input_masks, input_labels = batch\n",
    "            input_data = input_data.type(torch.LongTensor)\n",
    "            input_masks = input_masks.type(torch.LongTensor)\n",
    "            input_labels = input_labels.type(torch.LongTensor)\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # forward propagation\n",
    "            out = model(\n",
    "                input_data,\n",
    "                token_type_ids=None, \n",
    "                attention_mask=input_masks,\n",
    "                labels=input_labels\n",
    "            )\n",
    "            loss = out[0]\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss = total_loss/len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(\"Training took {}\".format(\n",
    "            (time.time() - start_train_time)\n",
    "        ))\n",
    "\n",
    "        # Validation\n",
    "        start_validation_time = time.time()\n",
    "        model.eval()\n",
    "        eval_loss, eval_acc = 0, 0\n",
    "        for step, batch in enumerate(test_loader):\n",
    "            eval_data, eval_masks, eval_labels = batch\n",
    "            eval_data = input_data.type(torch.LongTensor)\n",
    "            eval_masks = input_masks.type(torch.LongTensor)\n",
    "            eval_labels = input_labels.type(torch.LongTensor)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                out = model(\n",
    "                    eval_data,\n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=eval_masks\n",
    "                )\n",
    "            logits = out[0]\n",
    "\n",
    "            batch_acc = compute_accuracy(\n",
    "                logits.numpy(), eval_labels.numpy()\n",
    "            )\n",
    "\n",
    "            eval_acc += batch_acc\n",
    "            \n",
    "        print(\n",
    "            \"Accuracy: {}, Time elapsed: {}\".format(\n",
    "                eval_acc/(step + 1),\n",
    "                time.time() - start_validation_time\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 3 ========\n",
      "0/250 --> Time elapsed 0.007717132568359375\n",
      "10/250 --> Time elapsed 327.6781442165375\n",
      "20/250 --> Time elapsed 671.3720242977142\n",
      "30/250 --> Time elapsed 980.1099593639374\n",
      "40/250 --> Time elapsed 1277.7987241744995\n",
      "50/250 --> Time elapsed 1568.9109942913055\n",
      "60/250 --> Time elapsed 1894.9694511890411\n",
      "70/250 --> Time elapsed 2249.337110042572\n",
      "80/250 --> Time elapsed 2551.6829221248627\n",
      "90/250 --> Time elapsed 2854.640032052994\n",
      "100/250 --> Time elapsed 3155.7811381816864\n",
      "110/250 --> Time elapsed 3452.4280750751495\n",
      "120/250 --> Time elapsed 3757.349958181381\n",
      "130/250 --> Time elapsed 4064.8563373088837\n",
      "140/250 --> Time elapsed 4367.3598091602325\n",
      "150/250 --> Time elapsed 4682.460071325302\n",
      "160/250 --> Time elapsed 4979.263302326202\n",
      "170/250 --> Time elapsed 5297.050108194351\n",
      "180/250 --> Time elapsed 5601.208858251572\n",
      "190/250 --> Time elapsed 5905.388324260712\n",
      "200/250 --> Time elapsed 6215.239118099213\n",
      "210/250 --> Time elapsed 6516.767795085907\n",
      "220/250 --> Time elapsed 6821.974551200867\n",
      "230/250 --> Time elapsed 7131.477924108505\n",
      "240/250 --> Time elapsed 7434.527491092682\n",
      "Training took 7739.489291191101\n",
      "Accuracy: 0.96875, Time elapsed: 406.0266869068146\n",
      "======== Epoch 2 / 3 ========\n",
      "0/250 --> Time elapsed 0.0050466060638427734\n",
      "10/250 --> Time elapsed 306.3244597911835\n",
      "20/250 --> Time elapsed 606.5244836807251\n",
      "30/250 --> Time elapsed 972.6793847084045\n",
      "40/250 --> Time elapsed 1286.9958226680756\n",
      "50/250 --> Time elapsed 1616.0370829105377\n",
      "60/250 --> Time elapsed 1964.8659918308258\n",
      "70/250 --> Time elapsed 2366.735002756119\n",
      "80/250 --> Time elapsed 2719.9846317768097\n",
      "90/250 --> Time elapsed 3096.3182468414307\n",
      "100/250 --> Time elapsed 3417.6603016853333\n",
      "110/250 --> Time elapsed 3717.617649793625\n",
      "120/250 --> Time elapsed 4020.8391287326813\n",
      "130/250 --> Time elapsed 4319.785864830017\n",
      "140/250 --> Time elapsed 4623.491648674011\n",
      "150/250 --> Time elapsed 4928.384215831757\n",
      "160/250 --> Time elapsed 5238.179543733597\n",
      "170/250 --> Time elapsed 5538.141195774078\n",
      "180/250 --> Time elapsed 5841.162925720215\n",
      "190/250 --> Time elapsed 6135.661684989929\n",
      "200/250 --> Time elapsed 6435.780717849731\n",
      "210/250 --> Time elapsed 6734.230907917023\n",
      "220/250 --> Time elapsed 7145.571810722351\n",
      "230/250 --> Time elapsed 7475.176486730576\n",
      "240/250 --> Time elapsed 7785.218566894531\n",
      "Training took 8093.363415718079\n",
      "Accuracy: 1.0, Time elapsed: 400.8271930217743\n",
      "======== Epoch 3 / 3 ========\n",
      "0/250 --> Time elapsed 0.00632786750793457\n",
      "10/250 --> Time elapsed 298.2199077606201\n",
      "20/250 --> Time elapsed 592.4071187973022\n",
      "30/250 --> Time elapsed 894.68803191185\n",
      "40/250 --> Time elapsed 1205.5810387134552\n",
      "50/250 --> Time elapsed 1506.0359659194946\n",
      "60/250 --> Time elapsed 1803.6995429992676\n",
      "70/250 --> Time elapsed 2104.120475769043\n",
      "80/250 --> Time elapsed 2406.644169807434\n",
      "90/250 --> Time elapsed 2712.6301259994507\n",
      "100/250 --> Time elapsed 3017.915703058243\n",
      "110/250 --> Time elapsed 3331.389268875122\n",
      "120/250 --> Time elapsed 3679.584993839264\n",
      "130/250 --> Time elapsed 3993.732574939728\n",
      "140/250 --> Time elapsed 4299.100232839584\n",
      "150/250 --> Time elapsed 4604.583083868027\n",
      "160/250 --> Time elapsed 4927.064391851425\n",
      "170/250 --> Time elapsed 5239.181414842606\n",
      "180/250 --> Time elapsed 5541.667543888092\n",
      "190/250 --> Time elapsed 5921.572929859161\n",
      "200/250 --> Time elapsed 6432.481406927109\n",
      "210/250 --> Time elapsed 6980.637446880341\n",
      "220/250 --> Time elapsed 7525.695255994797\n",
      "230/250 --> Time elapsed 8075.607289075851\n",
      "240/250 --> Time elapsed 8747.21822476387\n",
      "Training took 9373.865983963013\n",
      "Accuracy: 1.0, Time elapsed: 705.2603988647461\n"
     ]
    }
   ],
   "source": [
    "losses = train_model(trainer, validator, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model\n",
    "model_to_save.save_pretrained(STORE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(new_df):\n",
    "    \"\"\"\n",
    "    Test the trained model on a dataset in another language.\n",
    "    This function assumes the input dataframe contains two columns\n",
    "    \"Reviews\" (the text of the review) and \"Labels\" (the score for\n",
    "    the review, where a 0 represents no complaint and a 1 represents\n",
    "    a complaint.)\n",
    "    \"\"\"\n",
    "    X = new_df[\"Reviews\"]\n",
    "    y = new_df[\"Label\"]\n",
    "\n",
    "    X_test_encoded = prep(X)\n",
    "    X_test_masks = attn_mask(X_test_encoded)\n",
    "\n",
    "    test_inputs = torch.tensor(X_test_encoded)\n",
    "    test_labels = torch.tensor(y.values)\n",
    "    test_masks = torch.tensor(X_test_masks)\n",
    "\n",
    "    test_data = TensorDataset(\n",
    "        test_inputs, \n",
    "        test_masks, \n",
    "        test_labels\n",
    "    )\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    tester = DataLoader(\n",
    "        test_data, \n",
    "        sampler=test_sampler, \n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    \n",
    "    for step, batch in enumerate(tester):\n",
    "        eval_data, eval_masks, eval_labels = batch\n",
    "        eval_data = eval_data.type(torch.LongTensor)\n",
    "        eval_masks = eval_masks.type(torch.LongTensor)\n",
    "        eval_labels = eval_labels.type(torch.LongTensor)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            out = model(\n",
    "                eval_data,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=eval_masks\n",
    "            )\n",
    "        logits = out[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        eval_labels = eval_labels.to('cpu').numpy()\n",
    "        batch_acc = compute_accuracy(logits, eval_labels)\n",
    "        eval_acc += batch_acc\n",
    "    print(\"Accuracy: {}\".format(eval_acc/(step + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9507053004396678\n"
     ]
    }
   ],
   "source": [
    "test_model(hindi_reviews)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
